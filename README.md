# Comparitive-Study-on-Fashion-MNIST-Dataset-based-on-various-Hyperparameters

Model Descriptions

Model 1:
● Activation function: ReLu
● Loss function: Categorical cross entropy
● Number of hidden layers: 1
● Number of nodes in hidden layer: 128
● Output layer: softmax

Model 2:
● Activation function: ReLu
● Loss function: Categorical cross entropy
● Number of hidden layers: 3
● Number of nodes in hidden layer: 128,64,32

● Hidden layer: ReLu
● Output layer: softmax

Model 3:
● Activation function: ReLu
● Loss function: Categorical cross entropy
● Number of hidden layers: 7
● Number of nodes in hidden layer: 1024, 512, 256, 128, 64, 32, 16
● Hidden layer:ReLu
● Output layer:Softmax

Model 4:
● Activation function: Sigmoid
● Loss function: Categorical Cross Entropy
● Number of hidden layers: 1
● Number of nodes in hidden layer:128
● Hidden layer: Sigmoid
● Output layer: Softmax

Model 5:
● Activation function: Sigmoid
● Loss function: Categorical cross entropy
● Number of hidden layers: 3
● Number of nodes in hidden layer: 128, 64,32
● Hidden layer: Sigmoid
● Output layer: Softmax

Model 6:
● Activation function: Sigmoid
● Loss function: Categorical Cross Entropy
● Number of hidden layers: 7
● Number of nodes in hidden layer:1024, 512, 256, 128, 64, 32, 16

● Hidden layer: Sigmoid
● Output layer: Softmax

Model 7:
● Activation function: tanh
● Loss function: Categorical Cross Entropy
● Number of hidden layers: 1
● Number of nodes in hidden layer: 128
● Hidden layer:tanh
● Output layer:Softmax

Model 8:
● Activation function: tanh
● Loss function: Categorical Cross Entropy
● Number of hidden layers:3
● Number of nodes in hidden layer:128,64,32
● Hidden layer: tanh
● Output layer:Softmax

Model 9:
● Activation function: tanh
● Loss function: Categorical Cross Entropy
● Number of hidden layers: 7
● Number of nodes in hidden layer:1024, 512, 256, 128, 64, 32, 16
● Hidden layer: tanh
● Output layer: softmax

Model 10:
● Activation function: ReLu
● Loss function:KL divergence
● Number of hidden layers:4
● Number of nodes in hidden layer: 128,64,32,16

● Hidden layer: ReLu
● Output layer: Softmax

Model 11:
● Activation function: ReLu
● Loss function: KL Divergence
● Number of hidden layers: 7
● Number of nodes in hidden layer: 1024, 512, 256, 128, 64, 32, 16
● Hidden layer: ReLu
● Output layer: Softmax

Model 12:
● Activation function: sigmoid
● Loss function: KL Divergence
● Number of hidden layers: 4
● Number of nodes in hidden layer: 128, 64, 32, 16
● Hidden layer: sigmoid
● Output layer:softmax

Model 13:
● Activation function: sigmoid
● Loss function: KL Divergence
● Number of hidden layers: 7
● Number of nodes in hidden layer: 1024, 512, 256, 128, 64, 32, 16
● Hidden layer: sigmoid
● Output layer: softmax

Model 14:
● Activation function: tanh
● Loss function: KL Divergence
● Number of hidden layers: 4
● Number of nodes in hidden layer: 128, 64, 32, 16

● Hidden layer: tanh
● Output layer:softmax

Model 15:
● Activation function:tanh
● Loss function: KL divergence
● Number of hidden layers: 7
● Number of nodes in hidden layer: 1024, 512, 256, 128, 64, 32, 16
● Hidden layer: tanh
● Output layer: softmax

Model 16:
● Activation function: tanh
● Loss function: KL Divergence
● Number of hidden layers: 5
● Number of nodes in hidden layer: 256, 128, 64, 32, 16
● Hidden layer: tanh
● Output layer: softmax
